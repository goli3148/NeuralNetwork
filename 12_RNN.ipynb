{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1DjVFrtxBH80"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torchvision import transforms\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import string\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7Qh4TjqHBH83"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Mr.J\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Mr.J\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Mr.J\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nlSeGm4eBH83"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7xwery8kBH83"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED  = 123\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "max_sequence    = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m8f8ZZerBH84"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>verified</th>\n",
              "      <th>reviewTime</th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>asin</th>\n",
              "      <th>style/Size:</th>\n",
              "      <th>style/Color:</th>\n",
              "      <th>reviewerName</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>summary</th>\n",
              "      <th>unixReviewTime</th>\n",
              "      <th>style/Size Name:</th>\n",
              "      <th>style/Style:</th>\n",
              "      <th>vote</th>\n",
              "      <th>image/0</th>\n",
              "      <th>image/1</th>\n",
              "      <th>image/2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1475</th>\n",
              "      <td>3</td>\n",
              "      <td>True</td>\n",
              "      <td>01 31, 2017</td>\n",
              "      <td>AODFGQL9CC7G7</td>\n",
              "      <td>B009MA34NY</td>\n",
              "      <td>7.5 B(M) US</td>\n",
              "      <td>Racer Blue/Obsidian/Blue Tint</td>\n",
              "      <td>ZS87</td>\n",
              "      <td>Squeaky</td>\n",
              "      <td>Three Stars</td>\n",
              "      <td>1485820800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>483</th>\n",
              "      <td>3</td>\n",
              "      <td>True</td>\n",
              "      <td>06 21, 2016</td>\n",
              "      <td>A2QJWHL4HY9FMF</td>\n",
              "      <td>B001IKJOLW</td>\n",
              "      <td>10 B(M) US</td>\n",
              "      <td>Black/Wolf Grey/White/Pink</td>\n",
              "      <td>Amazon Customer</td>\n",
              "      <td>Super cute shoe however runs wide and big, I'm...</td>\n",
              "      <td>Cute</td>\n",
              "      <td>1466467200</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1941</th>\n",
              "      <td>5</td>\n",
              "      <td>True</td>\n",
              "      <td>06 10, 2016</td>\n",
              "      <td>AIP4C9MG13COB</td>\n",
              "      <td>B0092UF54A</td>\n",
              "      <td>10.5 B(M) US</td>\n",
              "      <td>Black/White/Anthracite/Stealth</td>\n",
              "      <td>Cheryl A Stoneham</td>\n",
              "      <td>Comfortable and lightweight. Need a good train...</td>\n",
              "      <td>Comfortable and lightweight</td>\n",
              "      <td>1465516800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>682</th>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>05 19, 2017</td>\n",
              "      <td>AT7UEQ7UJB0RU</td>\n",
              "      <td>B0058YEJ5K</td>\n",
              "      <td>10 B(M) US</td>\n",
              "      <td>Cool Grey/Pure Platinum/White/Volt</td>\n",
              "      <td>J.T.</td>\n",
              "      <td>Love them absolutely comfortable. I'm very pic...</td>\n",
              "      <td>Amazing training Sneakers.</td>\n",
              "      <td>1495152000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
              "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
              "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>734</th>\n",
              "      <td>5</td>\n",
              "      <td>True</td>\n",
              "      <td>03 8, 2017</td>\n",
              "      <td>A1LLDHUO4OLXYI</td>\n",
              "      <td>B0058YEJ5K</td>\n",
              "      <td>10 B(M) US</td>\n",
              "      <td>Black/White/Anthracite/Stealth</td>\n",
              "      <td>mommyof2</td>\n",
              "      <td>Super comfy!!</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>1488931200</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      overall  verified   reviewTime      reviewerID        asin  \\\n",
              "1475        3      True  01 31, 2017   AODFGQL9CC7G7  B009MA34NY   \n",
              "483         3      True  06 21, 2016  A2QJWHL4HY9FMF  B001IKJOLW   \n",
              "1941        5      True  06 10, 2016   AIP4C9MG13COB  B0092UF54A   \n",
              "682         5     False  05 19, 2017   AT7UEQ7UJB0RU  B0058YEJ5K   \n",
              "734         5      True   03 8, 2017  A1LLDHUO4OLXYI  B0058YEJ5K   \n",
              "\n",
              "        style/Size:                         style/Color:       reviewerName  \\\n",
              "1475    7.5 B(M) US        Racer Blue/Obsidian/Blue Tint               ZS87   \n",
              "483      10 B(M) US           Black/Wolf Grey/White/Pink    Amazon Customer   \n",
              "1941   10.5 B(M) US       Black/White/Anthracite/Stealth  Cheryl A Stoneham   \n",
              "682      10 B(M) US   Cool Grey/Pure Platinum/White/Volt               J.T.   \n",
              "734      10 B(M) US       Black/White/Anthracite/Stealth           mommyof2   \n",
              "\n",
              "                                             reviewText  \\\n",
              "1475                                            Squeaky   \n",
              "483   Super cute shoe however runs wide and big, I'm...   \n",
              "1941  Comfortable and lightweight. Need a good train...   \n",
              "682   Love them absolutely comfortable. I'm very pic...   \n",
              "734                                       Super comfy!!   \n",
              "\n",
              "                          summary  unixReviewTime style/Size Name:  \\\n",
              "1475                  Three Stars      1485820800              NaN   \n",
              "483                          Cute      1466467200              NaN   \n",
              "1941  Comfortable and lightweight      1465516800              NaN   \n",
              "682    Amazing training Sneakers.      1495152000              NaN   \n",
              "734                    Five Stars      1488931200              NaN   \n",
              "\n",
              "     style/Style:  vote                                            image/0  \\\n",
              "1475          NaN   3.0                                                NaN   \n",
              "483           NaN   2.0                                                NaN   \n",
              "1941          NaN  10.0                                                NaN   \n",
              "682           NaN   4.0  https://images-na.ssl-images-amazon.com/images...   \n",
              "734           NaN   NaN                                                NaN   \n",
              "\n",
              "                                                image/1  \\\n",
              "1475                                                NaN   \n",
              "483                                                 NaN   \n",
              "1941                                                NaN   \n",
              "682   https://images-na.ssl-images-amazon.com/images...   \n",
              "734                                                 NaN   \n",
              "\n",
              "                                                image/2  \n",
              "1475                                                NaN  \n",
              "483                                                 NaN  \n",
              "1941                                                NaN  \n",
              "682   https://images-na.ssl-images-amazon.com/images...  \n",
              "734                                                 NaN  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('dataset/AMAZON_FASHION_5.csv')\n",
        "df = df.dropna(subset=['reviewText'])\n",
        "df = df.sample(frac=1, random_state=1)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    if isinstance(text, float):\n",
        "        print(text)\n",
        "    return text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
        "def remove_stop_words(tokens):\n",
        "    return [token for token in tokens if token not in stop_words]\n",
        "def lemmatize_words(tokens):\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wq2w62mmBH84"
      },
      "outputs": [],
      "source": [
        "review = df['reviewText'].apply(remove_punctuation).apply(word_tokenize).apply(remove_stop_words).apply(lemmatize_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PPTFtww6BH84"
      },
      "outputs": [],
      "source": [
        "def map_label(sentiment):\n",
        "    return sentiment-1\n",
        "\n",
        "sentiment = df['overall'].apply(map_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MlxRyX-GBH84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3160\n",
            "3160\n"
          ]
        }
      ],
      "source": [
        "X = review.to_list()\n",
        "y = sentiment[review.index].to_numpy()\n",
        "print(len(X))\n",
        "print(len(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XEm5RGJKBH85"
      },
      "outputs": [],
      "source": [
        "all_words = [token for phrase in X  for token in phrase ]\n",
        "vocab = Counter(all_words)\n",
        "vocab = sorted(vocab, key=vocab.get, reverse=True)\n",
        "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
        "encoded_review = [[vocab_to_int[token] for token in phrase] for phrase in X]\n",
        "\n",
        "\n",
        "\n",
        "features = np.zeros((len(encoded_review), max_sequence), dtype=np.int32)\n",
        "for i, e in enumerate(encoded_review):\n",
        "    e_len = len(e)\n",
        "    if e_len <= max_sequence:\n",
        "        zeros   = list(np.zeros(max_sequence-e_len))\n",
        "        new     = zeros + e\n",
        "    else:\n",
        "        new     = e[:max_sequence]\n",
        "    features[i,:] = np.array(new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HUKNxdPqBH85"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(features, y, test_size=.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DhceZ8v-BH85"
      },
      "outputs": [],
      "source": [
        "class imdbdataset(Dataset):\n",
        "    def __init__(self, x, y, transform=None) -> None:\n",
        "        super(imdbdataset, self).__init__()\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.x[index], self.y[index]\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample\n",
        "\n",
        "    # def get_labels(self):   return self.y\n",
        "\n",
        "class ToTensor:\n",
        "    def __call__(self, sample):\n",
        "        x, y = sample\n",
        "        x = np.array(x) if not isinstance(x, np.ndarray) else x\n",
        "        y = np.array(y) if not isinstance(y, np.ndarray) else y\n",
        "        return torch.from_numpy(x), torch.from_numpy(y).long()\n",
        "\n",
        "compose = transforms.Compose([\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "train_data_set = imdbdataset(x_train, y_train, transform=compose)\n",
        "test_data_set  = imdbdataset(x_test,  y_test,  transform=compose)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GJ0PMt_yBH85"
      },
      "outputs": [],
      "source": [
        "train_loader  = DataLoader(train_data_set, batch_size=16, shuffle=True)\n",
        "test_loader   = DataLoader(test_data_set,  batch_size=16, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "B5Czb6pwBH85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 100]) tensor([4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 1, 2, 4, 3])\n"
          ]
        }
      ],
      "source": [
        "example = iter(train_loader)\n",
        "feature, label = next(example)\n",
        "print(feature.shape, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "A757iEeoBH86"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, rnn_type='simple'):\n",
        "        super(RNN, self).__init__()\n",
        "        self.embedding  = nn.Embedding(input_dim, embedding_dim)\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "\n",
        "        if rnn_type == 'gru':\n",
        "            self.rnn        = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        elif rnn_type == 'lstm':\n",
        "            self.rnn        = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        else:\n",
        "            self.rnn        = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "        self.fc         = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, tt):\n",
        "        embedded = self.embedding(tt)\n",
        "\n",
        "        out, h = self.rnn(embedded)\n",
        "\n",
        "        if self.rnn_type == 'lstm':\n",
        "            h = h[0]\n",
        "\n",
        "        output = self.fc(h.squeeze_(0))\n",
        "        return output\n",
        "\n",
        "learning_rate   = .001\n",
        "num_epochs      = 3\n",
        "embedding_dim   = 100\n",
        "hidden_dim      = 128\n",
        "num_classes     = 5\n",
        "\n",
        "\n",
        "model = RNN(input_dim=len(vocab_to_int)+1, embedding_dim=embedding_dim,\n",
        "            hidden_dim=hidden_dim, output_dim=num_classes, rnn_type='lstm').to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8aSLzTRlBH86"
      },
      "outputs": [],
      "source": [
        "def test(class_test=False):\n",
        "    test_loss   = 0.0\n",
        "    total       = 0\n",
        "    correct     = 0\n",
        "    n_class_correct = [0 for i in range(num_classes)]\n",
        "    n_class_samples = [0 for i in range(num_classes)]\n",
        "    with torch.no_grad():\n",
        "        for text, label in test_loader:\n",
        "            text, label = text.to(device), label.to(device)\n",
        "            output = model(text)\n",
        "            loss = criterion(output, label)\n",
        "            test_loss += loss.item() * text.size(0)\n",
        "\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            total += label.size(0)\n",
        "            correct += (predicted == label).sum().item()\n",
        "            for i in range(len(label)):\n",
        "                label_ = label[i]\n",
        "                pred  = predicted[i]\n",
        "                if (label_ == pred):\n",
        "                    n_class_correct[label_] += 1\n",
        "                n_class_samples[label_] += 1\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = correct / total\n",
        "    print(f\" Test Loss: {test_loss}, Test Accuracy: {accuracy}\")\n",
        "    if class_test:\n",
        "        for i in range(num_classes):\n",
        "            acc = 100 * n_class_correct[i] / n_class_samples[i]\n",
        "            print(f'Accuracy of {i}:{acc}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "WqE2ED8JBH86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1/3 loss: 0.0537180266802824 Test Loss: 0.5189613858355752, Test Accuracy: 0.8306962025316456\n",
            "epoch 2/3 loss: 0.01656917480551461 Test Loss: 0.18245546538618546, Test Accuracy: 0.9509493670886076\n",
            "epoch 3/3 loss: 0.0045908226434854624 Test Loss: 0.1157798509620413, Test Accuracy: 0.9762658227848101\n",
            " Test Loss: 0.1157798509620413, Test Accuracy: 0.9762658227848101\n",
            "Accuracy of 0:100.0\n",
            "Accuracy of 1:100.0\n",
            "Accuracy of 2:89.70588235294117\n",
            "Accuracy of 3:94.25287356321839\n",
            "Accuracy of 4:99.29411764705883\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    for text, label in train_loader:\n",
        "        text, label = text.to(device), label.to(device)\n",
        "\n",
        "        output = model(text)\n",
        "\n",
        "        l = criterion(output, label)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        l.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += l.item()\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    print(f\"epoch {epoch+1}/{num_epochs} loss: {train_loss}\", end='')\n",
        "    test()\n",
        "test(class_test=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
